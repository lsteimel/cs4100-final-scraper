{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteimel/cs4100-final-scraper/blob/master/reranking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUpmSFflwzR1"
      },
      "source": [
        "# Reranking Retrieval Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2PzgU4YxIl3"
      },
      "source": [
        "In this notebook, you will continue using the [Pyserini](http://pyserini.io/) library's indexing and retrieval models.  This time, however, you will get an initial set of retrieval results and then write your own reranking code to try to move relevant documents higher in the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2vNd7bpJlDZ"
      },
      "source": [
        "As before, we start by installing the python interface. Since it calls the underlying Lucene search engine, which is written in Java, we make sure we point to an appropriate Java installation. If like Colab you don't have Java 21, uncomment the following code and run it, or whatever makes sense for your platform."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment the following code to install Java 21 on Colab\n",
        "!apt-get install openjdk-21-jre-headless -qq > /dev/null\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-21-openjdk-amd64/bin/java\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBDwrCizYdak",
        "outputId": "7e0498a6-871a-4f4e-e97f-09e1c1747131"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"21.0.6\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 21.0.6+7-Ubuntu-122.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 21.0.6+7-Ubuntu-122.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_lt0-pXJia0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab16c95d-5414-4433-9f7d-d14297a17764"
      },
      "source": [
        "!pip install pyserini\n",
        "# You can change this to gpu if you have one.\n",
        "# It's a pyserini dependency, but we won't need it until the next assignment.\n",
        "!pip install faiss-cpu"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.44.0.tar.gz (195.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.3/195.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pyserini) (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from pyserini) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pyserini) (2.32.3)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.11/dist-packages (from pyserini) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from pyserini) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (2.2.2)\n",
            "Collecting pyjnius>=1.6.0 (from pyserini)\n",
            "  Downloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.11/dist-packages (from pyserini) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyserini) (1.14.1)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (4.51.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (2.6.0+cu124)\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (1.75.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2 in /usr/local/lib/python3.11/dist-packages (from pyserini) (0.2.0)\n",
            "Collecting tiktoken>=0.4.0 (from pyserini)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: flask>3.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (3.1.0)\n",
            "Requirement already satisfied: pillow>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from pyserini) (11.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>3.0->pyserini) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>3.0->pyserini) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>3.0->pyserini) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>3.0->pyserini) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>3.0->pyserini) (1.9.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.8.1->pyserini) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.8.1->pyserini) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.8.1->pyserini) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.13.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.0.0->pyserini) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->pyserini) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->pyserini) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.4.0->pyserini) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pyserini) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pyserini) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pyserini) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pyserini) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->pyserini)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->pyserini) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.6.0->pyserini) (0.30.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.6.0->pyserini) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.6.0->pyserini) (0.5.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>3.0->pyserini) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->pyserini) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->pyserini) (1.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyjnius-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyserini\n",
            "  Building wheel for pyserini (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyserini: filename=pyserini-0.44.0-py3-none-any.whl size=195388880 sha256=4da4154a8c15e12da56a8ce6d1d2ae2e6c7dce543cb232d3bd3977e60e6e8be1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/fb/2c/523bde7d33a5dd4d0da19070ad3c96113f2b0face35a863f25\n",
            "Successfully built pyserini\n",
            "Installing collected packages: pyjnius, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12, pyserini\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.21.1 pyjnius-1.6.1 pyserini-0.44.0 tiktoken-0.9.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkD0kKxW9mHP"
      },
      "source": [
        "We initialize the searcher with a pre-built index for the Robust04 collection, which Pyserini will automatically download if it hasn't already. Note that the index takes up 1.6GB of disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVoAZvuAI_la",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80500d7-802d-4ef0-a974-f00f58c72614"
      },
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading index at https://rgw.cs.uwaterloo.ca/pyserini/indexes/lucene/lucene-inverted.disk45.20240803.36f7e3.tar.gz...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "lucene-inverted.disk45.20240803.36f7e3.tar.gz: 1.66GB [00:22, 79.9MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6xHyonHJDKy"
      },
      "source": [
        "Now we can search for a query and inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFZlcqEX0t1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f33dea-c136-4b09-ab12-c2010717cc3d"
      },
      "source": [
        "hits = searcher.search('black bear attacks', 1000)\n",
        "\n",
        "# Prints the first 10 hits\n",
        "for i in range(0, 10):\n",
        "    print(f'{i+1:2} {hits[i].docid:15} {hits[i].score:.5f}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1 LA092790-0015   7.06680\n",
            " 2 LA081689-0039   6.89020\n",
            " 3 FBIS4-16530     6.61630\n",
            " 4 LA102589-0076   6.46450\n",
            " 5 FT932-15491     6.25090\n",
            " 6 FBIS3-12276     6.24630\n",
            " 7 LA091090-0085   6.17030\n",
            " 8 FT922-13519     6.04270\n",
            " 9 LA052790-0205   5.94060\n",
            "10 LA103089-0041   5.90650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emoSTga_7fOB"
      },
      "source": [
        "The `IndexReaderUtils` class provides various methods to read the index directly. For example, we can fetch a raw document from the index given its `docid`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ApT1YG71mz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "855efc75-49a9-4bd4-8927-a4ea9a529b1d"
      },
      "source": [
        "from pyserini.index import LuceneIndexReader\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "reader = LuceneIndexReader.from_prebuilt_index('robust04')\n",
        "\n",
        "doc = reader.doc('LA092790-0015').raw()\n",
        "display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + doc + '</div>'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"font-family: Times New Roman; padding-bottom:10px\"><DATE>\n",
              "<P>\n",
              "September 27, 1990, Thursday, Ventura County Edition\n",
              "</P>\n",
              "</DATE>\n",
              "<HEADLINE>\n",
              "<P>\n",
              "HUNGRY WILDLIFE STRAYING INTO SUBURBS;\n",
              "</P>\n",
              "<P>\n",
              "DROUGHT: FOUR DRY YEARS HAVE PARCHED NATIVE VEGETATION, FORCING BOBCATS, BEARS,\n",
              "MOUNTAIN LIONS, DEER AND COYOTES TO FORAGE CLOSER TO INHABITED AREAS.\n",
              "</P>\n",
              "</HEADLINE>\n",
              "<TEXT>\n",
              "<P>\n",
              "Hungry bobcats, bears and mountain lions -- unable to find food in Ventura\n",
              "County's drought-parched forests -- are being pushed out of their natural\n",
              "habitats to scavenge in rural communities, game officials said Wednesday.\n",
              "</P>\n",
              "<P>\n",
              "Two weeks ago, a black bear ripped the door off a trailer home in Rose Valley\n",
              "just north of Ojai. And within the past month, there have been several reports\n",
              "of mountain lions eating livestock near Los Padres National Forest. Several\n",
              "bobcats have been reported near houses in the Ojai Valley.\n",
              "</P>\n",
              "<P>\n",
              "Authorities say that over the past two years they have received twice the\n",
              "complaints -- about 20 a month -- of wild animals in populated areas. The\n",
              "drought is now in its fourth year in California.\n",
              "</P>\n",
              "<P>\n",
              "\"We've been having more and more conflicts with animals,\" said Capt. Roger\n",
              "Reese, with the state Department of Fish and Game. \"The fact is, it's very dry\n",
              "out there, and there just isn't a lot of food and water for them.\"\n",
              "</P>\n",
              "<P>\n",
              "Animal control officials say they are advising residents in rural areas to be\n",
              "aware of the problem. But so far, no one has been attacked by the wild animals,\n",
              "although there have been such attacks reported elsewhere in Southern\n",
              "California, authorities said.\n",
              "</P>\n",
              "<P>\n",
              "Coyotes have been running amok, officials said. Virtually all parts of the\n",
              "county except beach areas probably have been visited at one time or another by\n",
              "coyotes, said Kathy Jenks, director of the Ventura County Department of Animal\n",
              "Regulation.\n",
              "</P>\n",
              "<P>\n",
              "In Ventura, coyotes are often seen in Grant Park above City Hall, and in Arroyo\n",
              "Verde Park in the Ondulando district on the east side, Jenks said.\n",
              "</P>\n",
              "<P>\n",
              "Elsewhere, coyotes have been seen on streets in Thousand Oaks, Moorpark and\n",
              "Simi Valley. The rural, foothill developments are especially vulnerable, she\n",
              "said.\n",
              "</P>\n",
              "<P>\n",
              "Jenks said she advises residents to keep small pets inside, especially at\n",
              "night.\n",
              "</P>\n",
              "<P>\n",
              "There have even been a few cases in which brazen coyotes have attacked family\n",
              "animals in back yards, and a large number of house cats are disappearing,\n",
              "officials said.\n",
              "</P>\n",
              "<P>\n",
              "\"The common house cat is like a fancy feast for a coyote,\" she said. \"They're\n",
              "hungry, they're thirsty and they're coming down out of the hills.\"\n",
              "</P>\n",
              "<P>\n",
              "There have been a few reports of deer grazing in people's yards, Reese said.\n",
              "</P>\n",
              "<P>\n",
              "Traditionally, September is the worst month for wildlife, authorities said.\n",
              "</P>\n",
              "<P>\n",
              "\"It is usually the driest month,\" Reese said. \"And a lot of animals that have\n",
              "been raised in the spring leave their parents and go in search of food.\"\n",
              "</P>\n",
              "<P>\n",
              "More animals are expected to leave their natural habitats if the drought\n",
              "continues, officials said.\n",
              "</P>\n",
              "<P>\n",
              "\"I tell people who call that if we didn't have the big cats and the coyotes, we\n",
              "would be overrun by rodents,\" Jenks said. \"I would much rather hear a coyote in\n",
              "the distance than have roof rats.\"\n",
              "</P>\n",
              "<P>\n",
              "Don DeBusschere, who lives on a 45-acre walnut orchard in Happy Valley near\n",
              "Ojai, said his family has grown accustomed to wild animals. DeBusschere said he\n",
              "has seen scores of coyotes, several deer and a black bear.\n",
              "</P>\n",
              "<P>\n",
              "Recently, he said, two bobcats have moved into the trees on the edge of his\n",
              "property.\n",
              "</P>\n",
              "<P>\n",
              "\"They're not out to get humans,\" DeBusschere said. \"They're just trying to make\n",
              "a living off the land.\"\n",
              "</P>\n",
              "</TEXT></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RMfMgWt8bgm"
      },
      "source": [
        "Note that the result is exactly the same as displaying the hit contents above. Given the raw text, we can obtain its analyzed form (i.e., tokenized, stemmed, stopwords removed, etc.). Here we show the first ten tokens:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzed = reader.analyze(doc)\n",
        "analyzed[0:10]"
      ],
      "metadata": {
        "id": "0opezXf5E0IW",
        "outputId": "0e498376-3ad6-4178-d6d7-6d15c6f1b54f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['date',\n",
              " 'p',\n",
              " 'septemb',\n",
              " '27',\n",
              " '1990',\n",
              " 'thursdai',\n",
              " 'ventura',\n",
              " 'counti',\n",
              " 'edit',\n",
              " 'p']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving Initial Ranked Lists\n",
        "\n",
        "We can load some standard evaluation sets such as Robust04, which contains 250 queries, or \"topics\" as the TREC conferences call them."
      ],
      "metadata": {
        "id": "GOFVMQKLyW1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search import get_topics\n",
        "topics = get_topics('robust04')\n",
        "print(f'{len(topics)} queries total')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1sdhbDTdwqf",
        "outputId": "7c8fc890-555a-4ff2-d0d0-c21fca62af17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250 queries total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics are in a dictionary, whose keys are integers uniquely identifying each query. Each topic contains the following fields:\n",
        "\n",
        "* `title`: TREC's term for the brief query a user might actually type;\n",
        "* `description`: a longer form of the query in the form of a complete sentence; and\n",
        "* `narrative`: a description of what the user is looking for and what kinds of results would be relevant or non-relevant."
      ],
      "metadata": {
        "id": "RbODj6sezBvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics[301]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBOeSkJxy-8R",
        "outputId": "6f10bd24-dd97-4a44-fe83-f795b1472229"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'narrative': 'A relevant document must as a minimum identify the organization and the type of illegal activity (e.g., Columbian cartel exporting cocaine). Vague references to international drug trade without identification of the organization(s) involved would not be relevant.',\n",
              " 'description': 'Identify organizations that participate in international criminal activity, the activity, and, if possible, collaborating organizations and the countries involved.',\n",
              " 'title': 'International Organized Crime'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of your experiments, we'll divide them into a development and test set."
      ],
      "metadata": {
        "id": "gMbgbZVqyzdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_topics = {k:topics[k] for k in list(topics.keys())[:125]}\n",
        "test_topics = {k:topics[k] for k in list(topics.keys())[125:]}"
      ],
      "metadata": {
        "id": "LILkqQDqd3Tj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll fetch the relevance judgments for the Robust04 queries, which TREC calls \"qrels\"."
      ],
      "metadata": {
        "id": "HTY9-DMyzuU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "qfile = 'https://github.com/castorini/anserini-tools/blob/63ceeab1dd94c1221f29b931d868e8fab67cc25c/topics-and-qrels/qrels.robust04.txt?raw=true'\n",
        "qrels = []\n",
        "for line in urlopen(qfile):\n",
        "  qid, round, docid, score = line.strip().split()\n",
        "  qrels.append([int(qid), 0, docid.decode('UTF-8'), int(score)])\n",
        "#qrels = [line.strip().split() for line in urlopen(qfile)]"
      ],
      "metadata": {
        "id": "b53vacvvf6fw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each record in the qrel contains four fields:\n",
        "\n",
        "1. the numeric identifier of the query;\n",
        "2. the round of relevance feedback, which is here always 0;\n",
        "3. the identifier of a documennt that has been judged; and\n",
        "4. the relevance score of that document.\n",
        "\n",
        "In Robust04, all relevance judgments are binary, i.e., 1 or 0. Note that not all non-relevant documents are recorded. The qrel file only contains those documents the annotators actually looked at; the vast majority of documents in the collection have not been judged. In IR evaluation, we assume that unannotated documents are non-relevant."
      ],
      "metadata": {
        "id": "BXg8YO590Aky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qrels[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJblOI_pgZBh",
        "outputId": "b7a9dd62-b5b9-42b0-c7bc-72fbc3f76ef9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[301, 0, 'FBIS3-10082', 1],\n",
              " [301, 0, 'FBIS3-10169', 0],\n",
              " [301, 0, 'FBIS3-10243', 1],\n",
              " [301, 0, 'FBIS3-10319', 0],\n",
              " [301, 0, 'FBIS3-10397', 1],\n",
              " [301, 0, 'FBIS3-10491', 1],\n",
              " [301, 0, 'FBIS3-10555', 0],\n",
              " [301, 0, 'FBIS3-10622', 1],\n",
              " [301, 0, 'FBIS3-10634', 0],\n",
              " [301, 0, 'FBIS3-10635', 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We collect the top 1000 hists for both the dev and test sets. You"
      ],
      "metadata": {
        "id": "aUJ9D9KMHVrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute top-1000 lists for queries in test_topics\n",
        "def topic_hits(searcher, topics, k=1000):\n",
        "  hits = {}\n",
        "  for topic, info in topics.items():\n",
        "    print(topic, info['title'])\n",
        "    hits[topic] = [(hit.docid, hit.score) for hit in searcher.search(info['title'], k)]\n",
        "  return hits\n",
        "\n",
        "dev_hits = topic_hits(searcher, dev_topics)\n",
        "test_hits = topic_hits(searcher, test_topics)"
      ],
      "metadata": {
        "id": "Ue-97Z5nGcH4",
        "outputId": "b1efd0a4-4f3b-4bba-c669-1ed26da0e37b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350 Health and Computer Terminals\n",
            "351 Falkland petroleum exploration\n",
            "352 British Chunnel impact\n",
            "353 Antarctica exploration\n",
            "354 journalist risks\n",
            "355 ocean remote sensing\n",
            "356 postmenopausal estrogen Britain\n",
            "357 territorial waters dispute\n",
            "358 blood-alcohol fatalities\n",
            "359 mutual fund predictors\n",
            "360 drug legalization benefits\n",
            "361 clothing sweatshops\n",
            "362 human smuggling\n",
            "363 transportation tunnel disasters\n",
            "364 rabies\n",
            "365 El Nino\n",
            "366 commercial cyanide uses\n",
            "367 piracy\n",
            "368 in vitro fertilization\n",
            "369 anorexia nervosa bulimia\n",
            "370 food/drug laws\n",
            "371 health insurance holistic\n",
            "372 Native American casino\n",
            "373 encryption equipment export\n",
            "374 Nobel prize winners\n",
            "375 hydrogen energy\n",
            "376 World Court\n",
            "377 cigar smoking\n",
            "378 euro opposition\n",
            "379 mainstreaming\n",
            "380 obesity medical treatment\n",
            "381 alternative medicine\n",
            "382 hydrogen fuel automobiles\n",
            "383 mental illness drugs\n",
            "384 space station moon\n",
            "385 hybrid fuel cars\n",
            "386 teaching disabled children\n",
            "387 radioactive waste\n",
            "388 organic soil enhancement\n",
            "389 illegal technology transfer\n",
            "700 gasoline tax U.S.\n",
            "390 orphan drugs\n",
            "391 R&D drug prices\n",
            "392 robotics\n",
            "393 mercy killing\n",
            "394 home schooling\n",
            "395 tourism\n",
            "396 sick building syndrome\n",
            "397 automobile recalls\n",
            "398 dismantling Europe's arsenal\n",
            "399 oceanographic vessels\n",
            "601 Turkey Iraq water\n",
            "602 Czech, Slovak sovereignty\n",
            "603 Tobacco cigarette lawsuit\n",
            "604 Lyme disease arthritis\n",
            "605 Great Britain health care\n",
            "606 leg traps ban\n",
            "607 human genetic code\n",
            "608 taxing social security\n",
            "609 per capita alcohol consumption\n",
            "610 minimum wage adverse impact\n",
            "611 Kurds Germany violence\n",
            "612 Tibet protesters\n",
            "613 Berlin wall disposal\n",
            "614 Flavr Savr tomato\n",
            "615 timber exports Asia\n",
            "616 Volkswagen Mexico\n",
            "617 Russia Cuba economy\n",
            "618 Ayatollah Khomeini death\n",
            "619 Winnie Mandela scandal\n",
            "620 France nuclear testing\n",
            "621 women ordained Church of England\n",
            "622 price fixing\n",
            "623 toxic chemical weapon\n",
            "624 SDI Star Wars\n",
            "625 arrests bombing WTC\n",
            "626 human stampede\n",
            "627 Russian food crisis\n",
            "628 U.S. invasion of Panama\n",
            "629 abortion clinic attack\n",
            "630 Gulf War Syndrome\n",
            "631 Mandela South Africa President\n",
            "632 southeast Asia tin mining\n",
            "633 Welsh devolution\n",
            "634 L-tryptophan deaths\n",
            "635 doctor assisted suicides\n",
            "636 jury duty exemptions\n",
            "637 human growth hormone (HGH)\n",
            "638 wrongful convictions\n",
            "639 consumer on-line shopping\n",
            "640 maternity leave policies\n",
            "641 Valdez wildlife marine life\n",
            "400 Amazon rain forest\n",
            "642 Tiananmen Square protesters\n",
            "401 foreign minorities, Germany\n",
            "643 salmon dams Pacific northwest\n",
            "402 behavioral genetics\n",
            "644 exotic animals import\n",
            "403 osteoporosis\n",
            "645 software piracy\n",
            "404 Ireland, peace talks\n",
            "646 food stamps increase\n",
            "405 cosmic events\n",
            "647 windmill electricity\n",
            "406 Parkinson's disease\n",
            "648 family leave law\n",
            "407 poaching, wildlife preserves\n",
            "649 computer viruses\n",
            "408 tropical storms\n",
            "409 legal, Pan Am, 103\n",
            "650 tax evasion indicted\n",
            "651 U.S. ethnic population\n",
            "410 Schengen agreement\n",
            "652 OIC Balkans 1990s\n",
            "411 salvaging, shipwreck, treasure\n",
            "653 ETA Basque terrorism\n",
            "412 airport security\n",
            "654 same-sex schools\n",
            "413 steel production\n",
            "655 ADD diagnosis treatment\n",
            "414 Cuba, sugar, exports\n",
            "656 lead poisoning children\n",
            "415 drugs, Golden Triangle\n",
            "657 school prayer banned\n",
            "416 Three Gorges Project\n",
            "658 teenage pregnancy\n",
            "417 creativity\n",
            "659 cruise health safety\n",
            "418 quilts, income\n",
            "419 recycle, automobile tires\n",
            "660 whale watching California\n",
            "661 melanoma treatment causes\n",
            "420 carbon monoxide poisoning\n",
            "662 telemarketer protection\n",
            "421 industrial waste disposal\n",
            "663 Agent Orange exposure\n",
            "301 International Organized Crime\n",
            "422 art, stolen, forged\n",
            "664 American Indian Museum\n",
            "302 Poliomyelitis and Post-Polio\n",
            "423 Milosevic, Mirjana Markovic\n",
            "665 poverty Africa sub-Sahara\n",
            "303 Hubble Telescope Achievements\n",
            "424 suicides\n",
            "666 Thatcher resignation impact\n",
            "304 Endangered Species (Mammals)\n",
            "425 counterfeiting money\n",
            "667 unmarried-partner households\n",
            "305 Most Dangerous Vehicles\n",
            "426 law enforcement, dogs\n",
            "668 poverty, disease\n",
            "306 African Civilian Deaths\n",
            "427 UV damage, eyes\n",
            "669 Islamic Revolution\n",
            "307 New Hydroelectric Projects\n",
            "428 declining birth rates\n",
            "308 Implant Dentistry\n",
            "429 Legionnaires' disease\n",
            "309 Rap and Crime\n",
            "670 U.S. elections apathy\n",
            "671 Salvation Army benefits\n",
            "430 killer bee attacks\n",
            "672 NRA membership profile\n",
            "310 Radio Waves and Brain Cancer\n",
            "431 robotic technology\n",
            "673 Soviet withdrawal Afghanistan\n",
            "311 Industrial Espionage\n",
            "432 profiling, motorists, police\n",
            "674 Greenpeace prosecuted\n",
            "312 Hydroponics\n",
            "433 Greek, philosophy, stoicism\n",
            "675 Olympics training swimming\n",
            "313 Magnetic Levitation-Maglev\n",
            "434 Estonia, economy\n",
            "676 poppy cultivation\n",
            "314 Marine Vegetation\n",
            "435 curbing population growth\n",
            "677 Leaning Tower of Pisa\n",
            "315 Unexplained Highway Accidents\n",
            "436 railway accidents\n",
            "678 joint custody impact\n",
            "316 Polygamy Polyandry Polygyny\n",
            "437 deregulation, gas, electric\n",
            "679 opening adoption records\n",
            "317 Unsolicited Faxes\n",
            "438 tourism, increase\n",
            "318 Best Retirement Country\n",
            "439 inventions, scientific discoveries\n",
            "319 New Fuel Sources\n",
            "680 immigrants Spanish school\n",
            "681 wind power location\n",
            "440 child labor\n",
            "682 adult immigrants English\n",
            "320 Undersea Fiber Optic Cable\n",
            "441 Lyme disease\n",
            "683 Czechoslovakia breakup\n",
            "321 Women in Parliaments\n",
            "442 heroic acts\n",
            "684 part-time benefits\n",
            "322 International Art Crime\n",
            "443 U.S., investment, Africa\n",
            "685 Oscar winner selection\n",
            "323 Literary/Journalistic Plagiarism\n",
            "444 supercritical fluids\n",
            "686 Argentina pegging dollar\n",
            "324 Argentine/British Relations\n",
            "445 women clergy\n",
            "687 Northern Ireland industry\n",
            "325 Cult Lifestyles\n",
            "446 tourists, violence\n",
            "688 non-U.S. media bias\n",
            "326 Ferry Sinkings\n",
            "447 Stirling engine\n",
            "689 family-planning aid\n",
            "327 Modern Slavery\n",
            "448 ship losses\n",
            "328 Pope Beatifications\n",
            "449 antibiotics ineffectiveness\n",
            "329 Mexican Air Pollution\n",
            "690 college education advantage\n",
            "691 clear-cutting forests\n",
            "450 King Hussein, peace\n",
            "692 prostate cancer detection treatment\n",
            "330 Iran-Iraq Cooperation\n",
            "693 newspapers electronic media\n",
            "331 World Bank Criticism\n",
            "694 compost pile\n",
            "332 Income Tax Evasion\n",
            "695 white collar crime sentence\n",
            "333 Antibiotics Bacteria Disease\n",
            "696 safety plastic surgery\n",
            "334 Export Controls Cryptography\n",
            "697 air traffic controller\n",
            "335 Adoptive Biological Parents\n",
            "698 literacy rates Africa\n",
            "336 Black Bear Attacks\n",
            "699 term limits\n",
            "337 Viral Hepatitis\n",
            "338 Risk of Aspirin\n",
            "339 Alzheimer's Drug Treatment\n",
            "340 Land Mine Ban\n",
            "341 Airport Security\n",
            "342 Diplomatic Expulsion\n",
            "343 Police Deaths\n",
            "344 Abuses of E-Mail\n",
            "345 Overseas Tobacco Sales\n",
            "346 Educational Standards\n",
            "347 Wildlife Extinction\n",
            "348 Agoraphobia\n",
            "349 Metabolism\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Initial Ranked Lists\n",
        "\n"
      ],
      "metadata": {
        "id": "LoDPnv1b04lP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When reranking, an important metric is the _recall_ of the initial set of results. This tells us the upper bound or &ldquo;headroom&rdquo; on the improvements that reranking can achieve. If the recall in the initial ranked lists is too low, we know we need to optimize the initial retrieval model.\n",
        "\n",
        "For this assignment, you will work with fixed initial ranked lists from pyserini's BM25 model, but it's still useful to see how much room there is for improvement during reranking.\n",
        "\n",
        "As before, you should process the `qrels` data to find the relevant results for each query."
      ],
      "metadata": {
        "id": "407VkQsK8h6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def organize_relevance_judgments(judgments):\n",
        "    relevance_data = {}\n",
        "    for judgment in judgments:\n",
        "        query_id = judgment[0]\n",
        "        doc_id = judgment[2]\n",
        "        relevance_score = judgment[3]\n",
        "\n",
        "        relevance_data.setdefault(query_id, {})\n",
        "        relevance_data[query_id][doc_id] = relevance_score\n",
        "    return relevance_data\n",
        "\n",
        "def calculate_recall_metrics(hits_data, relevance_data):\n",
        "    recall_metrics = {}\n",
        "    for query_id, retrieved_documents in hits_data.items():\n",
        "        if query_id in relevance_data:\n",
        "            relevant_document_set = {doc_id for doc_id, score in relevance_data[query_id].items() if score > 0}\n",
        "            retrieved_document_ids = {doc_id for doc_id, _ in retrieved_documents}\n",
        "            retrieved_relevant = retrieved_document_ids.intersection(relevant_document_set)\n",
        "\n",
        "            total_relevant = len(relevant_document_set)\n",
        "            recall_value = len(retrieved_relevant) / total_relevant if total_relevant > 0 else 0.0\n",
        "            recall_metrics[query_id] = recall_value\n",
        "    return recall_metrics\n",
        "\n",
        "def calculate_average_metric(metrics_dict):\n",
        "    return sum(metrics_dict.values()) / len(metrics_dict) if metrics_dict else 0\n",
        "\n",
        "\n",
        "relevance_data = organize_relevance_judgments(qrels)\n",
        "\n",
        "development_recall_metrics = calculate_recall_metrics(dev_hits, relevance_data)\n",
        "test_recall_metrics = calculate_recall_metrics(test_hits, relevance_data)\n",
        "\n",
        "dev = calculate_average_metric(development_recall_metrics)\n",
        "test = calculate_average_metric(test_recall_metrics)\n",
        "\n",
        "print(f\"Recall@1000, dev queries: {dev:.4f}\")\n",
        "print(f\"Recall@1000, test queries: {test:.4f}\")"
      ],
      "metadata": {
        "id": "aGQK-C935EVA",
        "outputId": "5b72b5ae-9da0-4a0c-dd6b-9cd1c9a37c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@1000, dev queries: 0.6985\n",
            "Recall@1000, test queries: 0.6993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a given set of top-1000 lists, Recall@1000 will not change after reranking. What will change are ranking-based metrics like MAP and NDCG. You should compute MAP@1000 for the initial `dev_hits` and `test_hits` data."
      ],
      "metadata": {
        "id": "zVcipvGDKd0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_precision(result_documents, ground_truth_relevant, cutoff=1000):\n",
        "    if len(ground_truth_relevant) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    docs_retrieved = [doc_tuple[0] for doc_tuple in result_documents[:cutoff]]\n",
        "\n",
        "    relevant_found = 0\n",
        "    precision_sum = 0.0\n",
        "\n",
        "    for position, doc_id in enumerate(docs_retrieved, 1):\n",
        "        if doc_id in ground_truth_relevant:\n",
        "            relevant_found += 1\n",
        "            current_precision = relevant_found / position\n",
        "            precision_sum += current_precision\n",
        "\n",
        "    return precision_sum / len(ground_truth_relevant)\n",
        "\n",
        "\n",
        "def evaluate_mean_average_precision(search_results, relevance_judgments, max_depth=1000):\n",
        "    precision_values = []\n",
        "\n",
        "    for query_id, ranked_docs in search_results.items():\n",
        "        if query_id not in relevance_judgments:\n",
        "            continue\n",
        "\n",
        "        relevant_set = set()\n",
        "        for document_id, relevance in relevance_judgments[query_id].items():\n",
        "            if relevance > 0:\n",
        "                relevant_set.add(document_id)\n",
        "\n",
        "        query_ap = compute_average_precision(ranked_docs, relevant_set, max_depth)\n",
        "        precision_values.append(query_ap)\n",
        "\n",
        "    if not precision_values:\n",
        "        return 0.0\n",
        "\n",
        "    return np.mean(precision_values)\n",
        "\n",
        "\n",
        "dev_map = evaluate_mean_average_precision(\n",
        "    dev_hits, relevance_data, max_depth=1000)\n",
        "test_map = evaluate_mean_average_precision(\n",
        "    test_hits, relevance_data, max_depth=1000)\n",
        "\n",
        "print(f\"MAP@1000 for dev queries (initial): {dev_map:.4f}\")\n",
        "print(f\"MAP@1000 for test queries (initial): {test_map:.4f}\")"
      ],
      "metadata": {
        "id": "-jQYNXRnK6DL",
        "outputId": "60c707d8-c0de-44f4-8044-3a2af2cffe69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@1000 for dev queries (initial): 0.2426\n",
            "MAP@1000 for test queries (initial): 0.2637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking Search Results\n",
        "\n",
        "In this final part of the assignment, you should implement a ranking function that, hopefully, improves on the baseline BM25 ranking. You may use the BM25 score for each document as input, as well as the query, of course, and any other properties of the documents you look up with the `reader` object.  After computing a new score for each candidate, re-sort the top-1000 results by your model's score.\n",
        "\n",
        "You may use anything you've learned in this course---or in another course---to build your ranking function. For example, you could implement pseudo-relevance feedback or a relevance model, which would treat the top of each ranked list (e.g., the top 100) as if it were truly relevant and retrain model parameters. You could tune different BM25, query likelihood, or sequential dependence model parameters. You could try to learn different weights or embeddings for different fields in documents. You could use implementations of transformer language models such as BERT or SentenceBERT to score the compatibility of queries and documents. To be clear, you don't have to any of these approaches; you are free to try whatever ideas you like.\n",
        "\n",
        "If your reranking model has tunable parameters, you should tune them on the `dev_hits` set. In the end, you will also evaluate MAP@1000 on the `test_hits` set."
      ],
      "metadata": {
        "id": "MpdecWki1sxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Put any explanation of your reranking function here."
      ],
      "metadata": {
        "id": "_mzX46Kj8Vsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "K_PSEUDO_RELEVANT = 10\n",
        "NUM_EXPANSION_TERMS = 20\n",
        "\n",
        "\n",
        "def get_rocchio_expanded_query(topic_id, initial_hits, reader, topics, k_pseudo, num_expansion_terms):\n",
        "    original_query_text = topics[topic_id]['title']\n",
        "    analyzed_original_query = reader.analyze(original_query_text)\n",
        "\n",
        "    k_pseudo = min(k_pseudo, len(initial_hits))\n",
        "    if k_pseudo == 0:\n",
        "        return original_query_text\n",
        "\n",
        "    pseudo_relevant_docids = [doc_id for doc_id,\n",
        "                              score in initial_hits[:k_pseudo]]\n",
        "\n",
        "    term_freqs = Counter()\n",
        "    for doc_id in pseudo_relevant_docids:\n",
        "        try:\n",
        "            doc_vector = reader.get_document_vector(doc_id)\n",
        "            if doc_vector:\n",
        "                term_freqs.update(doc_vector)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    if not term_freqs:\n",
        "        return original_query_text\n",
        "\n",
        "    expansion_terms = []\n",
        "    for term, freq in term_freqs.most_common():\n",
        "        if term not in analyzed_original_query:\n",
        "            expansion_terms.append(term)\n",
        "            if len(expansion_terms) >= num_expansion_terms:\n",
        "                break\n",
        "\n",
        "\n",
        "    expanded_query_text = original_query_text + \" \" + \" \".join(expansion_terms)\n",
        "    return expanded_query_text\n",
        "\n",
        "\n",
        "def rerank_hits(searcher, reader, topics, initial_hits_map, k_pseudo, num_expansion, k_rerank=1000):\n",
        "    reranked_hits_map = {}\n",
        "    original_doc_ids_map = {topic_id: {doc_id for doc_id, score in hits}\n",
        "                            for topic_id, hits in initial_hits_map.items()}\n",
        "\n",
        "    total_topics = len(initial_hits_map)\n",
        "    processed_topics = 0\n",
        "\n",
        "    for topic_id, initial_hits in initial_hits_map.items():\n",
        "        processed_topics += 1\n",
        "\n",
        "        if not initial_hits:\n",
        "            reranked_hits_map[topic_id] = []\n",
        "            continue\n",
        "\n",
        "        expanded_query = get_rocchio_expanded_query(\n",
        "            topic_id, initial_hits, reader, topics, k_pseudo, num_expansion)\n",
        "\n",
        "        try:\n",
        "            new_search_results = searcher.search(\n",
        "                expanded_query, k=int(k_rerank * 1.5))\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error searching expanded query for topic {topic_id}: {e}. Falling back to initial ranking.\")\n",
        "\n",
        "            reranked_hits_map[topic_id] = initial_hits\n",
        "            continue\n",
        "\n",
        "        original_doc_ids = original_doc_ids_map[topic_id]\n",
        "        final_reranked_dict = {}\n",
        "        found_original_ids = set()\n",
        "\n",
        "        for hit in new_search_results:\n",
        "            if hit.docid in original_doc_ids:\n",
        "                final_reranked_dict[hit.docid] = hit.score\n",
        "                found_original_ids.add(hit.docid)\n",
        "\n",
        "        min_score = -1e9\n",
        "        missed_docs = original_doc_ids - found_original_ids\n",
        "        for doc_id in missed_docs:\n",
        "            final_reranked_dict[doc_id] = min_score\n",
        "\n",
        "        final_reranked_list = list(final_reranked_dict.items())\n",
        "        final_reranked_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "        reranked_hits_map[topic_id] = final_reranked_list[:k_rerank]\n",
        "\n",
        "    return reranked_hits_map\n",
        "\n",
        "best_k_pseudo = K_PSEUDO_RELEVANT\n",
        "best_num_expansion = NUM_EXPANSION_TERMS\n",
        "\n",
        "reranked_dev_hits = rerank_hits(\n",
        "    searcher, reader, dev_topics, dev_hits, best_k_pseudo, best_num_expansion)\n",
        "reranked_test_hits = rerank_hits(\n",
        "    searcher, reader, test_topics, test_hits, best_k_pseudo, best_num_expansion)"
      ],
      "metadata": {
        "id": "MkKXkWQj7mrh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "map_dev_reranked = evaluate_mean_average_precision  (\n",
        "    reranked_dev_hits, relevance_data, max_depth=1000)\n",
        "print(f\"MAP@1000 for dev queries (reranked): {map_dev_reranked:.4f}\")\n",
        "\n",
        "map_test_reranked = evaluate_mean_average_precision(\n",
        "    reranked_test_hits, relevance_data, max_depth=1000)\n",
        "print(f\"MAP@1000 for test queries (reranked): {map_test_reranked:.4f}\")\n",
        "\n",
        "print(f\"MAP@1000 for dev queries (initial):  {dev_map:.4f}\")\n",
        "print(f\"MAP@1000 for test queries (initial): {test_map:.4f}\")\n",
        "\n",
        "dev_improvement = map_dev_reranked - dev_map\n",
        "test_improvement = map_test_reranked - test_map\n",
        "print(f\"Improvement on dev set:  {dev_improvement:+.4f}\")\n",
        "print(f\"Improvement on test set: {test_improvement:+.4f}\")"
      ],
      "metadata": {
        "id": "me4Pnxgw79dE",
        "outputId": "d663f18c-2d7e-4d49-c1ce-91f2d8524d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP@1000 for dev queries (reranked): 0.2287\n",
            "MAP@1000 for test queries (reranked): 0.2151\n",
            "MAP@1000 for dev queries (initial):  0.2426\n",
            "MAP@1000 for test queries (initial): 0.2637\n",
            "Improvement on dev set:  -0.0139\n",
            "Improvement on test set: -0.0486\n"
          ]
        }
      ]
    }
  ]
}